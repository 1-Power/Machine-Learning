{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification:\n",
    "- When there aare only two output either yes or no then this kind of classification is called Binary classification.\n",
    "- There will be only two type of output.\n",
    "- They can be yes or no 1 or 0 respectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "- Logictic Regression is very similar to the Linerar Regression but with logical regression we compute the probability and determine the 1 or 0.\n",
    "- There is a important function caleed the sigmod function or the logistic fucntion\n",
    "- The sigmod function output the value between 0 and 1. \n",
    "- The formula for sigmod function is \n",
    "  *** y = 1 / (1 + exp(-x)) ***\n",
    "and there is an image that is stated bellow\n",
    "\n",
    "![sigmod-function](./img/sigma%20function.png)\n",
    "\n",
    "\n",
    "- When z is very large then the value of g(z) will be very close to 1.\n",
    "- IF the value of Z is very small then the value of g(z) will be close to 0\n",
    "\n",
    "![sigmod-function](./img/logistical-regression-cal.png)\n",
    "\n",
    "![sigmod-function](./img/cal2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Boundary\n",
    "- It the line when the vlaues are spred o each side of x and y and there is a solid line.\n",
    "- TO the right side there will be 1 values and on the left side there can be 0 Values or vice versa. \n",
    "- That boundary is called Decision Boundary.\n",
    "- Sometime descision Boundary can not be stright then in that case uou need to decide the ratioo pint where the point lay and then adjust to get the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function for Logistic Regression:\n",
    "- The formaula for Cost function in Linear regress can not be used because when you make the graph of the cost function it will be a convex function and it will be a very complex function and the value of gradient desent will not be correct. \n",
    "- There will be a greater lose just checkout the bellow diagram.\n",
    "\n",
    "![image](./img/Cost-Function-Logistical-Regression.png)\n",
    "\n",
    "- As The second graph shows that the value od the gradient desent is decressed at certain points and the value of the gradient desent is increased at certain points if you are using the cost function of the linear regression. so we need to change ad twee the cost function for the logistic regression.\n",
    "\n",
    "- The valie is will always going to be in between 1 and 0.\n",
    "- If the Probalily os close to 0 ypur model can be correct.\n",
    "- As shown in the bellow diagram.\n",
    "\n",
    "![image](./img/Lost-function-Logistic-Regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Desent in Logistic Regression:\n",
    "- Just like Linear Regression there is Gradient despent for the Logistic Regression and we have to find the value of the w and b. with the learning rate so the formula for that is stated bellow.\n",
    "\n",
    "![img](./img/Gradent-Desent-Logistical%20Regression.png)\n",
    "\n",
    "\n",
    "- As you see the value value of the cost function and the learn Regression Gradient is being used and with some change we can get the value of the Logistic Regression Gradient Desent.\n",
    "- But that will raise some more questions that if the formula is same looking so that graph will be same along without as well.\n",
    "- The answer is no the graph will not be same because the value of the cost function is not same as the Linear Regression because the cost functiona value is not same as the Linear Regression and in logistical Regress thw value will be in between the 1 and 0 so that is why the graph will not be same along with the output.\n",
    "- In learn Regress the value of f(x) is different and for Logistic Regression the value of f(x) is different and both are stared bellow:\n",
    "\n",
    "![image](./img/smae-equation-logistical-Regression.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is not the linear regression because the formula and defition is not the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistial Regression in Python:\n",
    "- This is the code from start to end for the Logistic Regression in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# logistic regression\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'loss: {self.__loss(h, y)} \\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        return self.predict_prob(X) >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0152412]\n",
      "<bound method LogisticRegression.predict_prob of <__main__.LogisticRegression object at 0x000002BF84937D00>>\n"
     ]
    }
   ],
   "source": [
    "# example data\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "# fit the model to the data\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# predict probabilities for new data\n",
    "print(model.predict_prob(np.array([[1, 1]]))) # 0.5\n",
    "print(model.predict_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting\n",
    "### Difference between Overfitting and Underfitting:\n",
    "In machine learning, underfitting and overfitting are two common problems that can occur when training a model. Underfitting occurs when a model is not able to capture the underlying pattern in the data and therefore performs poorly on both training and test data. This can happen when the model is too simple or when there is not enough data to train the model. On the other hand, overfitting occurs when a model performs well on the training data but poorly on test data. This happens when the model is too complex and has learned the noise or random fluctuations in the training data, rather than the underlying pattern.\n",
    "\n",
    "To avoid underfitting and overfitting, it is important to select a model that is neither too simple nor too complex for the data at hand. This can be achieved through techniques such as cross-validation, where the model is trained on different subsets of the data and evaluated on the remaining data, or by using regularization techniques, which constrain the model to prevent it from becoming too complex. Additionally, collecting more data can also help improve the performance of the model and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is high variance in machine learning?\n",
    "In machine learning, high variance refers to a model that has a large amount of error or deviation from the true underlying relationship in the data. This means that the model is overly sensitive to the specific training data and performs poorly on new or unseen data.\n",
    "\n",
    "High variance is often caused by a model that is too complex for the given data. For example, a model with many parameters or degrees of freedom will tend to overfit to the training data and will not generalize well to new data. This can be observed by comparing the model's performance on the training data (which is typically very good) and on test data (which is typically much worse).\n",
    "\n",
    "To address high variance, it is necessary to find a balance between model complexity and the amount of training data available. This can be achieved through techniques such as regularization, which constrains the model to prevent it from becoming too complex, or by collecting more data to train the model. It is also important to evaluate the model on multiple metrics, such as the error on the training data and the error on test data, to ensure that the model is not overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image of Overfitting and Underfitting:\n",
    "![](./img/Overfitting%26Underfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Overfitting:\n",
    "Overfitting is a common problem in machine learning, where a model performs well on the training data but poorly on new or unseen data. This happens when the model is too complex and has learned the noise or random fluctuations in the training data, rather than the underlying pattern. To address overfitting, there are several techniques that can be used:\n",
    "\n",
    "- Collect more data: Overfitting is more likely to occur when the training data is limited or biased. By collecting more data, especially data that is representative of the real-world application of the model, it is possible to train a more generalizable model that can better handle new or unseen data.\n",
    "\n",
    "- Use regularization: Regularization is a technique that constrains the model to prevent it from becoming too complex. This can be achieved through methods such as L1 or L2 regularization, which add a penalty term to the loss function based on the magnitude of the model weights, or by using dropout, which randomly drops some neurons from the model during training to prevent them from co-adapting.\n",
    "\n",
    "- Early stopping: Early stopping is a regularization technique that involves monitoring the model's performance on a validation set during training and stopping the training process when the performance on the validation set starts to decrease. This can prevent the model from overfitting to the training data by limiting the number of training iterations.\n",
    "\n",
    "- Cross-validation: Cross-validation is a model evaluation technique that involves dividing the data into multiple sets, training the model on different combinations of the data, and evaluating the model on the remaining data. This can provide a more accurate estimate of the model's performance on new or unseen data and can help to identify overfitting.\n",
    "\n",
    "By using these techniques, it is possible to address overfitting and train more robust and generalizable machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2508f8e729a69d123a52de305d944e7cafa7ec1a9867a508d1b9b3663af4f26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
