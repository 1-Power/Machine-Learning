{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model:\n",
    "- Training the model and then give us the correct predictions.\n",
    "- Predict a number from a database this is called Regress Model.\n",
    "- Linear Regression Model is a Regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning Set:\n",
    "-  The model will learn from the data to get the accuracy of the output predictions.\n",
    "- x = \"input\" variable\n",
    "- y = \"output\" variable\n",
    "- m = number of training examples\n",
    "- So the formula for the training set will be (x,y) and each row will be represented by an exponential but the power will not be included.\n",
    "so x^2 = not x^2 not exponential it repeseent a row number.\n",
    "\n",
    "- A Training set have two things;\n",
    "1. Input features variable (x)\n",
    "2. Output variable (y)\n",
    "\n",
    "Bothing of these thing will be feed into the learning algorithm. the learning algorithm will learn from the data and then give us the correct predictions that predication will be called a ***function*** and the function will be called ***hypothesis***.\n",
    "<br>The Job of this function is to take another input x and then give us the correct prediction. This output will be called y^ (y-hat). y^ is called the predacted output of y. The value of y^ may not be correct because it's a prediction.\n",
    "<br> The perfect way to represent this function is f(x)=wx+b, where as w and b are numbers and the number choosen by the w and by will determin the prediction of y^.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function:\n",
    "- w and b are the parameters of the function.\n",
    "- these are the weights or coffecients of the function.\n",
    "- We use these parameters during training to improve the model. \n",
    "- To Identify which weight is good we use the ***cost function***.\n",
    "- The cost function compare the value of y^ with the value of y. by taking out the (y^ -y)^2 . this is called the error. We are checking how far are we from the correct answer. \n",
    "- TO check the error in the training set the equation will be sum of (y^ -y)^2 will m and m is the number of training. this is called the cost function.\n",
    "- The value of cost function will increase with the values of data so to counter that we take the average of the cost function. so the equation will be 1/2m sum of ((y^ -y)^2 )1/ 2m .\n",
    "\n",
    "- This is also called the squared error function. and the formula will be J(w,b)= 1/2m (sum of ( y^ -y)^2 ) .\n",
    "- Cost function measures how a machine learning model performs. Cost function is the calculation of the error between predicted values and actual values, represented as a single real number\n",
    "\n",
    "- The main purpose of cost function is to check the performace of the model. how is the performace of the model.\n",
    "- For the Linear regression model, the cost function will be the minimum of the Root Mean Squared Error of the model, obtained by subtracting the predicted values from actual values\n",
    "- The Lower the value of J(x) or the cost function better the performace of the model. \n",
    "- But Tweaking the values of w and b can be difficult in large set of data so to deal with that we have to use an other model called Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent:\n",
    "- Gradient Descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
    "- Cosider your self on the top of the hill and you want to go down and you don't know whoch way to go, you will spin 360 degree and then check for surronding and then you will go down, Once your are down then you spin again and will at a point you will reach the bottom of the hill that will be the value of w. then you will run the entire process again, you will find the value of b both of these value will be local mininum. \n",
    "- The formaula for gradient descent to find the value of w  is ***w= w-alpha (d/dx){j(w,b)}***.\n",
    "- ***Where alpha is the learning rate***.\n",
    "- Greater the value of alpha the bigger then you will be take huge steps down from the moutain and if the value is too small then you will be taking small steps and it will take a long time to reach the bottom of the hill.\n",
    "- We need to choose a good Learning rate so that the model is working well.\n",
    "- In the formula is very simple the you need to derative of the cost function. \n",
    "- For a model to work you need with two varaibles and so we have compute w and now it's time to compute b.\n",
    "- For b the formula will be ***b= b-alpha (d/dx){j(w,b)}***.\n",
    "- You must update the value of b and w at the same time. it is important \n",
    "- In order to get the value of d/dx we create a slop and then based on that slop we can compulte the value of the w and b. \n",
    "- In an example let's asume that the value of b is zero and the graph is like a U shap so the gradient decent will be like on the right it will be postive and on the left the value will be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate:\n",
    "- Learning Rate is Important because the learning rate can make or break the gradient decent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7e04bacde83c45f056322cab5a9ea9be3fef8ab73c979275119ef699b5265a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
